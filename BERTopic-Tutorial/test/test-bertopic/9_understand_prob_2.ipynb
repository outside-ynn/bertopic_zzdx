{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers.pipelines import pipeline\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 加载文件\n",
    "with open('../../data/切词.txt', 'r', encoding='utf-8') as file:\n",
    "  docs = file.readlines()\n",
    "print('条数: ', len(docs))\n",
    "print('预览第一条: ', docs[0])\n",
    "\n",
    "vectorizer_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 词向量模型，同时加载本地训练好的词向量\n",
    "embedding_model = pipeline(\"feature-extraction\", model=\"bert-base-chinese\") # 使用bert-base-chinese\n",
    "embeddings = np.load('../../data/embedding_bbc.npy') # 使用bert-base-chinese向量\n",
    "print(embeddings.shape)\n",
    "\n",
    "# 2. 创建分词模型\n",
    "vectorizer_model = CountVectorizer() # 因为我们已经分好词了，所以这里不需要传入分词函数了\n",
    "\n",
    "# 3. 创建UMAP降维模型\n",
    "umap_model = UMAP(\n",
    "  n_neighbors=15,\n",
    "  n_components=5,\n",
    "  min_dist=0.0,\n",
    "  metric='cosine',\n",
    "  random_state=42  # ⚠️ 防止随机 https://maartengr.github.io/BERTopic/faq.html\n",
    ")\n",
    "\n",
    "# 4. 创建HDBSCAN聚类模型\n",
    "# 如果要建设离群值，可以减小下面两个参数\n",
    "# https://hdbscan.readthedocs.io/en/latest/faq.html\n",
    "hdbscan_model = HDBSCAN(\n",
    "  min_cluster_size=20,\n",
    "  min_samples=5,\n",
    "  prediction_data=True # 一定要注意，如果后文设置了calculate_probabilities = True，一定要设置该参数 https://github.com/MaartenGr/BERTopic/issues/1103\n",
    ")\n",
    "\n",
    "# 5. 创建CountVectorizer模型\n",
    "vectorizer_model = CountVectorizer(stop_words=['洛阳', '旅游', '文化'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  # 官方文档\n",
    "  # The calculate_probabilties parameter is only used when using HDBSCAN or cuML's HDBSCAN model. In other words, this will not work when using a model other than HDBSCAN\n",
    "\n",
    "  # 源码注释\n",
    "  # https://maartengr.github.io/BERTopic/faq.html#how-do-i-calculate-the-probabilities-of-all-topics-in-a-document\n",
    "  # calculate_probabilities: Calculate the probabilities of all topics\n",
    "  #                        per document instead of the probability of the assigned\n",
    "  #                        topic per document. This could slow down the extraction\n",
    "  #                        of topics if you have many documents (> 100_000). \n",
    "  #                        NOTE: If false you cannot use the corresponding\n",
    "  #                        visualization method `visualize_probabilities`.\n",
    "  #                        NOTE: This is an approximation of topic probabilities\n",
    "  #                        as used in HDBSCAN and not an exact representation.\n",
    "  # 简而言之，这个值默认是False\n",
    "  # 它的含义是：如果设置为False（默认值），则只计算一条文档归属于其所属主题的概率；如果设置为True则计算一条文档归属于所有主题的概率。但可能会降低运算效率\n",
    "  # 其次，该参数仅用于使用HDBSCAN做聚类的场景\n",
    "  calculate_probabilities = True\n",
    ")\n",
    "\n",
    "# 源码注释\n",
    "# predictions: Topic predictions for each documents\n",
    "# probabilities: The probability of the assigned topic per document.\n",
    "#   If `calculate_probabilities` in BERTopic is set to True, then\n",
    "#   it calculates the probabilities of all topics across all documents\n",
    "#   instead of only the assigned topic. This, however, slows down\n",
    "#   computation and may increase memory usage.\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embeddings) #传入训练好的词向量\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先看一个topic_model.fit_transform输出的是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topics), topics[:10])\n",
    "print(len(probs), probs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "topic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
