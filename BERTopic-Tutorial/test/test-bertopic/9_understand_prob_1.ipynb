{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers.pipelines import pipeline\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 加载文件\n",
    "with open('../../data/切词.txt', 'r', encoding='utf-8') as file:\n",
    "  docs = file.readlines()\n",
    "print('条数: ', len(docs))\n",
    "print('预览第一条: ', docs[0])\n",
    "\n",
    "vectorizer_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 词向量模型，同时加载本地训练好的词向量\n",
    "embedding_model = pipeline(\"feature-extraction\", model=\"bert-base-chinese\") # 使用bert-base-chinese\n",
    "embeddings = np.load('../../data/embedding_bbc.npy') # 使用bert-base-chinese向量\n",
    "print(embeddings.shape)\n",
    "\n",
    "# 2. 创建分词模型\n",
    "vectorizer_model = CountVectorizer() # 因为我们已经分好词了，所以这里不需要传入分词函数了\n",
    "\n",
    "# 3. 创建UMAP降维模型\n",
    "umap_model = UMAP(\n",
    "  n_neighbors=15,\n",
    "  n_components=5,\n",
    "  min_dist=0.0,\n",
    "  metric='cosine',\n",
    "  random_state=42  # ⚠️ 防止随机 https://maartengr.github.io/BERTopic/faq.html\n",
    ")\n",
    "\n",
    "# 4. 创建HDBSCAN聚类模型\n",
    "# 如果要建设离群值，可以减小下面两个参数\n",
    "# https://hdbscan.readthedocs.io/en/latest/faq.html\n",
    "hdbscan_model = HDBSCAN(\n",
    "  min_cluster_size=20,\n",
    "  min_samples=5,\n",
    ")\n",
    "\n",
    "# 5. 创建CountVectorizer模型\n",
    "vectorizer_model = CountVectorizer(stop_words=['洛阳', '旅游', '文化'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    ")\n",
    "\n",
    "# 源码注释\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embeddings) #传入训练好的词向量\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先看一个topic_model.fit_transform输出的是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topics), topics[:10])\n",
    "print(len(probs), probs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "topic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
